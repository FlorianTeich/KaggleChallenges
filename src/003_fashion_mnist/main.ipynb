{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "from torch.utils.data import DataLoader\n",
    "import kcu as utils\n",
    "import time\n",
    "import pandas as pd\n",
    "from tpot import TPOTClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import decomposition\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "from kcu import capsules\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import gzip\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_mnist(path, kind='train'):\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = os.path.join(path,\n",
    "                               '%s-labels-idx1-ubyte.gz'\n",
    "                               % kind)\n",
    "    images_path = os.path.join(path,\n",
    "                               '%s-images-idx3-ubyte.gz'\n",
    "                               % kind)\n",
    "\n",
    "    with gzip.open(labels_path, 'rb') as lbpath:\n",
    "        labels = np.frombuffer(lbpath.read(), dtype=np.uint8,\n",
    "                               offset=8)\n",
    "\n",
    "    with gzip.open(images_path, 'rb') as imgpath:\n",
    "        images = np.frombuffer(imgpath.read(), dtype=np.uint8,\n",
    "                               offset=16).reshape(len(labels), 784)\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "\"\"\" SETUP \"\"\"\n",
    "dry_run = True\n",
    "cwdir = os.getcwd()\n",
    "\n",
    "train_X, train_Y = load_mnist(cwdir + '/../../data/fashion', kind='train')\n",
    "test_X, test_Y = load_mnist(cwdir + '/../../data/fashion', kind='t10k')\n",
    "\n",
    "# Split train set into train and validation\n",
    "train_inds, val_inds = sklearn.model_selection.train_test_split(\n",
    "    np.arange(len(train_Y)), test_size=0.2\n",
    ")\n",
    "train_X, val_X = train_X[train_inds], train_X[val_inds]\n",
    "train_Y, val_Y = train_Y[train_inds], train_Y[val_inds]\n",
    "\n",
    "if dry_run:\n",
    "    train_X, train_Y = train_X[:1024], train_Y[:1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" DATA EXPLORATION \"\"\"\n",
    "# Visualize class distribution\n",
    "utils.visualization.show_class_distribution(train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# As we know this is 2D image data, visualize some samples:\n",
    "plt.imshow(train_X[0].reshape(28, 28), cmap=\"binary\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Show an average of multiple instances from same class (class 1)\n",
    "for i in range(10):\n",
    "    inds = np.where(train_Y == i)\n",
    "    acc = np.mean(train_X[inds], axis=0)\n",
    "    plt.imshow(acc.reshape(28, 28), cmap=\"binary\")\n",
    "    plt.title(\"Mean values for instances of Class \" + str(i))\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Correlation analysis: for each class, lets identify important pixels first:\n",
    "for i in range(10):\n",
    "    new_Y = train_Y == i\n",
    "    corr_coeff = pd.DataFrame(np.hstack([train_X, np.expand_dims(new_Y, 1)])).corr()[-1:].to_numpy()[0, :-1]\n",
    "    plt.imshow(corr_coeff.reshape(28, 28), cmap=\"PiYG\")\n",
    "    plt.title(\"Correlations of pixels with target class \" + str(i))\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Lets do PCA: identifying a good linear combination of features that maximize the total variance\n",
    "pca = decomposition.PCA(n_components=2)\n",
    "view = pca.fit_transform(train_X)\n",
    "plt.scatter(view[:,0], view[:,1], c=train_Y, alpha=0.5, cmap='Set1')\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"PCA scatter plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Lets have a look at what features/pixels were used the most by our two PCA components:\n",
    "for i in [0,1]:\n",
    "    plt.imshow(pca.components_[i].reshape(28,28), cmap=\"PiYG\")\n",
    "    plt.title(\"PCA, linear combination for Component \" + str(i))\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "# There are a lot of pixels with no to only a bit of weight, so we could do feature selection and simply neglect pixels\n",
    "# e.g. in the corners of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Next, we visualize the t-SNE embedding. The desirable outcome here would be to have compact non-overlapping\n",
    "# clusters of classes. This plot might already foreshadow - to some extent - how complicated the classification\n",
    "# task will be\n",
    "view = TSNE(n_components=2, random_state=0).fit_transform(train_X)\n",
    "#plt.figure(figsize=(20,10))\n",
    "plt.scatter(view[:,0], view[:,1], c=train_Y, alpha=0.5, cmap=\"Set1\")\n",
    "plt.xlabel('Component 1')\n",
    "plt.ylabel('Component 2')\n",
    "plt.title(\"t-SNE embedding\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" MAIN CLASSIFICATION PIPELINES \"\"\"\n",
    "\n",
    "train_X, train_Y = load_mnist(cwdir + '/../../data/fashion', kind='train')\n",
    "test_X, test_Y = load_mnist(cwdir + '/../../data/fashion', kind='t10k')\n",
    "\n",
    "train_inds, val_inds = sklearn.model_selection.train_test_split(\n",
    "    np.arange(len(train_Y)), test_size=0.2\n",
    ")\n",
    "train_X, val_X = train_X[train_inds], train_X[val_inds]\n",
    "train_Y, val_Y = train_Y[train_inds], train_Y[val_inds]\n",
    "\n",
    "# Try several classifiers using kFold CrossValidation\n",
    "performances = utils.boilerplates.run_several_classifiers(train_X, train_Y, cv=True, use_gpu_methods=False)\n",
    "\n",
    "# Lets report their performances\n",
    "chart = sns.boxplot(x=\"method\", y=\"accuracy\", data=performances)\n",
    "chart.set_xticklabels(chart.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
    "plt.title(\"Accuracy of various methods using 5-fold CV\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Also, its interesting to see how well the classifiers do with less data\n",
    "all_perfs = pd.DataFrame(columns=[\"method\", \"balanced_accuracy\", \"num_samples\"])\n",
    "possible_samples = [128, 256, 512, 1024]\n",
    "for samples in possible_samples:\n",
    "    performances = utils.boilerplates.run_several_classifiers(train_X[:samples], train_Y[:samples], cv=True)\n",
    "    performances[\"num_samples\"] = samples\n",
    "    all_perfs = all_perfs.append(performances)\n",
    "\n",
    "chart = sns.lineplot(x=\"num_samples\", y=\"accuracy\", hue=\"method\", data=all_perfs)\n",
    "#chart.set_xticklabels(possible_samples)\n",
    "chart.set_xticks(possible_samples)\n",
    "chart.set_xticklabels(possible_samples)\n",
    "plt.title(\"Effect of train set on accuracy of various methods\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Lets train an MLP (as they scale better than SVMs with the amount of training data)\n",
    "clf = MLPClassifier()\n",
    "clf.fit(train_X, train_Y)\n",
    "pred = clf.predict(val_X)\n",
    "score = sklearn.metrics.accuracy_score(val_Y, pred)\n",
    "print(\"MLP Val accuracy: \" + str(score))\n",
    "\n",
    "# Plot confusion matrix of best classifier\n",
    "sklearn.metrics.plot_confusion_matrix(clf,\n",
    "                      val_X,\n",
    "                      val_Y,\n",
    "                      cmap=\"Reds\")\n",
    "plt.title(\"MLP confusion matrix on validation data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Lets leverage the local spatial relations of the input data and use a 2D CNN\n",
    "train_dataset = utils.dataset.MNISTDataset(train_X, train_Y)\n",
    "val_dataset = utils.dataset.MNISTDataset(val_X, val_Y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "cnn = utils.models.MnistCnn01().to(device)\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=0.001)\n",
    "utils.boilerplates.train_classifier(\n",
    "    cnn, optimizer, train_loader, device, 10, nn.CrossEntropyLoss(), val_loader, show_plot=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# We could do some hyperparameter tuning now by doing GridSearch, however I rather want to check out\n",
    "# one particular Neural Network that I worked with during my PhD: Capsule Networks\n",
    "capsule_net = capsules.CapsuleNetwork().to(device)\n",
    "criterion = capsules.CapsuleLoss()\n",
    "optimizer = optim.Adam(capsule_net.parameters())\n",
    "\n",
    "def train(capsule_net, criterion, optimizer, n_epochs=10, print_every=300):\n",
    "    losses = []\n",
    "    for epoch in range(1, n_epochs):\n",
    "        train_loss = 0.0\n",
    "        capsule_net.train()\n",
    "        for batch_i, (images, target) in tqdm(enumerate(train_loader)):\n",
    "            target = torch.eye(10).index_select(dim=0, index=target)\n",
    "            images, target = images.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            caps_output, reconstructions, y = capsule_net(images)\n",
    "            loss = criterion(caps_output, target, images, reconstructions)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            if batch_i != 0 and batch_i % print_every == 0:\n",
    "                avg_train_loss = train_loss/print_every\n",
    "                losses.append(avg_train_loss)\n",
    "                print('Epoch: {} \\tTraining Loss: {:.8f}'.format(epoch, avg_train_loss))\n",
    "                train_loss = 0\n",
    "        if epoch % 1 == 0:\n",
    "            out = []\n",
    "            gt = []\n",
    "            capsule_net.eval()\n",
    "            for image, target in tqdm(val_loader):\n",
    "                image = image.to(device)\n",
    "                caps_out, reconstructed, y = capsule_net(image)\n",
    "                _, pred = torch.max(y.data.cpu(), 1)\n",
    "                out.extend(pred.numpy().tolist())\n",
    "                gt.extend(target.numpy().tolist())\n",
    "\n",
    "            print(\"Test Accuracy:\", sklearn.metrics.accuracy_score(gt, out))\n",
    "\n",
    "    return losses\n",
    "\n",
    "losses = train(capsule_net, criterion, optimizer, n_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
