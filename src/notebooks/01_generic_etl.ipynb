{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generic ETL Pipeline Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/21 13:59:53 WARN Utils: Your hostname, florian-G11CD-K resolves to a loopback address: 127.0.1.1; using 192.168.178.39 instead (on interface enp3s0)\n",
      "23/05/21 13:59:53 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "23/05/21 13:59:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import dill\n",
    "import pyspark\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "sys.path.append(os.path.join(os.getcwd(), '..'))\n",
    "from kcu.functiontransform import FunctionTransform\n",
    "from pathlib import Path\n",
    "\n",
    "sqlurl = \"sqlite:///\" +  os.getcwd() + \"/save_pandas.db\"\n",
    "table = \"person_data\"\n",
    "pluginpath = os.getcwd() + \"/../../plugins\"\n",
    "\n",
    "engine = sqlalchemy.create_engine(sqlurl, echo=False)\n",
    "sqlite_connection = engine.connect()\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"id\": [0, 1, 2],\n",
    "    \"alter\": [3.4, 0.3, 7.2]\n",
    "})\n",
    "\n",
    "df.to_sql(table, sqlite_connection, if_exists=\"replace\")\n",
    "\n",
    "ids = df.id.tolist()\n",
    "\n",
    "sess = SparkSession.builder \\\n",
    "            .config(\n",
    "                \"spark.jars\",\n",
    "                \"{}/sqlite-jdbc-3.34.0.jar\".format(pluginpath)) \\\n",
    "            .config(\n",
    "                \"spark.driver.extraClassPath\",\n",
    "                \"{}/sqlite-jdbc-3.34.0.jar\".format(pluginpath)) \\\n",
    "           .getOrCreate()\n",
    "\n",
    "df = sess.read.format('jdbc') \\\n",
    "        .options(driver='org.sqlite.JDBC', dbtable=table,\n",
    "                 url='jdbc:' + sqlurl) \\\n",
    "        .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_add_some(df, id, id_column, feature_column, some,\n",
    "                  out_feature_column):\n",
    "    import pyspark.sql.functions as F\n",
    "\n",
    "    def df_from_list(sess, payload={}, add_index=True):\n",
    "        temp_data = []\n",
    "        temp_schema = []\n",
    "        if add_index:\n",
    "            temp_schema.append(\"idx_\")\n",
    "            temp_data.append([i for i in range(len(list(payload.values())[0]))])\n",
    "        temp_data.extend(payload.values())\n",
    "        temp_schema.extend(list(payload.keys()))\n",
    "        return sess.createDataFrame(data=list(zip(*temp_data)), schema=temp_schema)\n",
    "\n",
    "    anchor_df = df_from_list(\n",
    "        sess,\n",
    "        payload={\n",
    "            id_column: id,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    df = (\n",
    "        anchor_df.join(\n",
    "            df,\n",
    "            anchor_df[id_column] == df[id_column],\n",
    "            how=\"left\",\n",
    "        )\n",
    "        .drop(anchor_df[id_column])\n",
    "    ).withColumn(out_feature_column, F.col(feature_column) + F.lit(some))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'default_value': <function feature_add_some at 0x7f768373bf40>, 'parameter_value': {'feature_column': 'alter', 'id_column': 'id', 'some': 1, 'id': [0, 1, 2], 'out_feature_column': 'alter_postprocessed'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+-----+-------------------+\n",
      "|idx_|index| id|alter|alter_postprocessed|\n",
      "+----+-----+---+-----+-------------------+\n",
      "|   0|    0|  0|  3.4|                4.4|\n",
      "|   1|    1|  1|  0.3|                1.3|\n",
      "|   2|    2|  2|  7.2|                8.2|\n",
      "+----+-----+---+-----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_alter_parameters = lambda ids: {\n",
    "                \"feature_column\": \"alter\",\n",
    "                \"id_column\": \"id\",\n",
    "                \"some\": 1,\n",
    "                \"id\": ids,\n",
    "                \"out_feature_column\": \"alter_postprocessed\"}\n",
    "\n",
    "feature_param_pairs = [\n",
    "    (feature_add_some, feature_alter_parameters)\n",
    "]\n",
    "\n",
    "stages = []\n",
    "dict_to_save = dict()\n",
    "\n",
    "for fpp in feature_param_pairs:\n",
    "    ft = FunctionTransform(\n",
    "        default_value=fpp[0],\n",
    "        parameter_value=fpp[1](ids)\n",
    "    )\n",
    "    stages.append(ft)\n",
    "    dict_to_save[ft.uid] = fpp[1]\n",
    "\n",
    "pipe = Pipeline(stages=stages)\n",
    "newpipe = pipe.fit(df)\n",
    "newpipe.write().overwrite().save(\"models/testpipe\")\n",
    "\n",
    "newpipe.transform(df).show()\n",
    "\n",
    "with open('fpp.pickle', 'wb') as handle:\n",
    "    dill.dump(dict_to_save, handle, protocol=dill.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KERNEL NEUSTARTEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/21 14:02:30 WARN Utils: Your hostname, florian-G11CD-K resolves to a loopback address: 127.0.1.1; using 192.168.178.39 instead (on interface enp3s0)\n",
      "23/05/21 14:02:30 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "23/05/21 14:02:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "FunctionTransform_04cb01fe8f38\n",
      "+----+-----+---+-----+-------------------+\n",
      "|idx_|index| id|alter|alter_postprocessed|\n",
      "+----+-----+---+-----+-------------------+\n",
      "|   0|    2|  2|  7.2|                8.2|\n",
      "|   1|    0|  0|  3.4|                4.4|\n",
      "+----+-----+---+-----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import dill\n",
    "import pyspark\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "sys.path.append(os.path.join(os.getcwd(), '..'))\n",
    "from kcu.functiontransform import FunctionTransform\n",
    "\n",
    "\n",
    "sqlurl = \"sqlite:///\" +  os.getcwd() + \"/save_pandas.db\"\n",
    "table = \"person_data\"\n",
    "pluginpath = os.getcwd() + \"/../../plugins\"\n",
    "\n",
    "\n",
    "sess = SparkSession.builder \\\n",
    "            .config(\n",
    "                \"spark.jars\",\n",
    "                \"{}/sqlite-jdbc-3.34.0.jar\".format(pluginpath)) \\\n",
    "            .config(\n",
    "                \"spark.driver.extraClassPath\",\n",
    "                \"{}/sqlite-jdbc-3.34.0.jar\".format(pluginpath)) \\\n",
    "           .getOrCreate()\n",
    "\n",
    "df = sess.read.format('jdbc') \\\n",
    "        .options(driver='org.sqlite.JDBC', dbtable=table,\n",
    "                 url='jdbc:' + sqlurl) \\\n",
    "        .load()\n",
    "\n",
    "ids = [2, 0]\n",
    "\n",
    "newpipe = PipelineModel.load(\"models/testpipe\")\n",
    "\n",
    "with open('fpp.pickle', 'rb') as handle:\n",
    "    fpp = dill.load(handle)\n",
    "\n",
    "for entry in fpp:\n",
    "    print(entry)\n",
    "    for i in range(len(newpipe.stages)):\n",
    "        if newpipe.stages[i].uid == entry:\n",
    "            newpipe.stages[i] = newpipe.stages[i].setParameterValue(dill.dumps(fpp[entry](ids)).decode(encoding=\"raw_unicode_escape\"))\n",
    "            \n",
    "transformed = newpipe.transform(df)\n",
    "\n",
    "transformed.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dstoolbox-nkr8lXNm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
